配置4台主机，两台作为NameNode，两台作为DataNode，其中三台配置JournalNode，三台配置Zookeeper

修改主机名和ip：
修改主机/etc/hostname 为 master，master-back，slave1，slave2......
修改主机从机/etc/hosts，添加对应IP地址：
192.168.1.191	master
192.168.1.192	slave1
192.168.1.193	slave2
192.168.1.194 master-back 
......

一、主机从机添加用户组(root下)：
sudo groupadd hadoop
sudo useradd -s /bin/bash -d /home/hduser -m hduser -g hadoop
sudo passwd hduser
sudo usermod -G wheel hduser

赋予用户sudo权限：
chmod 777 /etc/sudoers
vi /etc/sudoers
增加：hduser    ALL=(ALL)       ALL
pkexec chmod 555 /etc/sudoers

设置静态IP：
vi /etc/sysconfig/network-scripts/ifcfg-eth0
ONBOOT=yes
BOOTPROTO=static
IPADDR=192.168.31.181
NETMASK=255.255.255.0
切换hduser用户：
su hduser

二、ssh免密登陆：
主机:
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa 生成秘钥

主机 and 从机：
sudo cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 认证公钥

希望ssh公钥生效需满足至少下面两个条件：
　　　　　　1) .ssh目录的权限必须是700   
sudo chmod 700 -R .ssh/
　　　　　　2) .ssh/authorized_keys文件权限必须是600
sudo chmod 600 .ssh/authorized_keys

永久关闭防火墙命令。(防火墙不关闭，节点之间无法通信)
sudo systemctl stop firewalld.service   
sudo systemctl disable firewalld.service

设置权限：
sudo vi /etc/ssh/sshd_config
打开
RSAAuthentication yes
PubkeyAuthentication yes

ssh master第一次输入密码后，exit退出，第二次登录实现免密。

三、安装配置hadoop：
1、拷贝环境到主机：
cp hadoop-2.8.5.tar.gz ~/
cp jdk-linux-x64.tar.gz ~/
2、解压：
tar -zxvf hadoop-2.8.5.tar.gz;tar -zxvf jdk-linux-x64.tar.gz

3、创建快捷方式：
sudo ln -snf /home/hduser/hadoop/hadoop-2.7.3 /opt/hadoop
sudo ln -snf /home/hduser/hadoop/jdk1.8.0_144/ /opt/jdk
4、设置data目录
sudo mkdir -p /home/hduser/data/hadoop/hdfs/namenode
sudo mkdir -p /home/hduser/data/hadoop/hdfs/datanode
sudo mkdir -p /home/hduser/data/hadoop/yarn/namenode
sudo mkdir -P /home/hduser/data/tmp
一定要设置成：
sudo chown -R hduser /home/hduser/data
sudo chown -R hduser /home/hduser/hadoop

配置环境变量：
vi ~/.bashrc

export JAVA_HOME=/opt/jdk
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin
echo "config java env......"
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
echo "config hadoop env......"

source ~/.bashrc
                        
5、配置hadoop目录下的几个文件及环境变量主要有这5个文件需要修改：
（1）、etc/hadoop/hadoop-env.sh 添加：
export JAVA_HOME=/opt/jdk

（2）、etc/hadoop/core-site.xml 添加：
<configuration>
  <property>
		<name>fs.defaultFS</name>
	  <value>hdfs://mycluster</value>  
    <description>HDFS入口</description>
	</property>
	<property>
    <name>ha.zookeeper.quorum</name>
    <value>master-back:2181,slave1:2181,slave2:2181</value>
    <description>设置自动故障转移的机器，即只有这三台机器需要安装Zookeeper</description>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hduser/data/ha</value>
    <description>tmp文件路径</description>
	</property>
</configuration>

（3）、etc/hadoop/hdfs-site.xml 添加：
<configuration>
	<property>
  	<name>dfs.nameservices</name>
    <value>mycluster</value>
    <description>设置主节点名称</description>
	</property>
	<property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
    <description>设置主备节点ID</description>
	</property>
	<property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>master:8020</value>
    <description>设置主节点RPC地址</description>
	</property>
	<property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>master-back:8020</value>
    <description>设置备份节点RPC地址</description>
	</property>
	<property>
		<name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>master:50070</value>
    <description>设置主节点访问地址</description>
	</property>
	<property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>master-back:50070</value>
    <description>设置备份节点访问地址</description>
	</property>
 	<property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://master:8485;master-back:8485;slave1:8485/mycluster</value>
    <description>设置journalnode地址，即日志共享节点地址</description>
	</property>
	<property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    <description>故障切换调用接口</description>
	</property>
	<property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
    <description>故障隔离方法</description>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/home/hduser/.ssh/id_rsa</value>
    <description>故障隔离需要私钥登陆</description>
  </property>
	<property>
  	<name>dfs.journalnode.edits.dir</name>
  	<value>/home/hduser/data/ha/journalnode</value>
    <description>设置journalnode存放文件夹</description>
	</property>
	<property>
   	<name>dfs.ha.automatic-failover.enabled</name>
   	<value>true</value>
    <description>设置自动故障切换使能</description>
 	</property>
	<property>
    <name>dfs.replication</name>
		<value>3</value>
    <description>设置block备份数量</description>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:///home/hduser/data/hadoop/hdfs/namenode</value>
    <description>设置NameNode路径</description>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:///home/hduser/data/hadoop/hdfs/datanode</value>
    <description>设置DataNode路径</description>
	</property>
</configuration>

（4）、etc/hadoop/mapred-site.xml 添加：
注意：如果在刚解压之后，没有这个文件,要将mapred-site.xml.template复制为mapred-site.xml。
cp hadoop-2.8.5/etc/hadoop/mapred-site.xml.template hadoop-2.8.5/etc/hadoop/mapred-site.xml

<configuration>
	<property>
		<!--指定Mapreduce运行在yarn上-->
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
</configuration>

（5）、etc/hadoop/yarn-site.xml 添加：
<configuration>
	<!-- 指定reducer获取数据的方式-->
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.ha.enabled</name>
  		<value>true</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.cluster-id</name>
  		<value>cluster1</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.ha.rm-ids</name>
  		<value>rm1,rm2</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.hostname.rm1</name>
  		<value>master</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.hostname.rm2</name>
  		<value>master-back</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.webapp.address.rm1</name>
  		<value>master:8088</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.webapp.address.rm2</name>
  		<value>master-back:8088</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.zk-address</name>
  		<value>master-back:2181,slave1:2181,slave2:2181</value>
	</property>
</configuration>

（6）、etc/hadoop/slaves 添加：
slave1
slave2

6、克隆虚拟机
(1)、修改对应主机名
(2)、修改对应ip地址

7、安装Zookeeper，只有（master-bac，slave1，slave2需要安装Zookeeper）
配置参考hadoop/core-site.xml的ha.zookeeper.quorum参数
（1）、解压zookeeper-3.4.10.tar.gz：
tar -zxvf zookeeper-3.4.10.tar.gz

（2）、修改配置文件
cd zookeeper-3.4.10/conf/
rm ../docs/ -rf  #删除文档，加快拷贝速度。
cp zoo_sample.cfg zoo.cfg   #zookeeper只能识别zoo.cfg，如果不修改运行的时候必须指定配置文件
vi zoo.cfg
修改配置文件为：

#心跳间隔，默认单位ms，也是zookeeper里面最小的时间单位
#还可以通过心跳来控制Flower跟Leader的通信时间，默认情况下FL的会话时常是心跳间隔的两倍。
tickTime=2000

# zookeeper里最大的初始化同步时间限制为10个tickTime即20s
initLimit=10

#zookeeper里leader和follower检测的最大延迟
syncLimit=5

#事务日志存储路径
dataDir=/home/hduser/data/zookeeper

#客户端和服务端连接的端口
clientPort=2181

# 客户端连接的最大数量
#maxClientCnxns=60

#设置服务器名称和地址，2888服务器之间通信端口，3888服务器之间选举端口
server.1=master-back:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888

（3）、配置系统环境变量
vi ~/.bashrc
添加Zookeeper环境变量：
export JAVA_HOME=/opt/jdk
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib
#export PATH=$PATH:$JAVA_HOME/bin
echo "config java env......"
export HADOOP_HOME=/opt/hadoop
#export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
echo "config hadoop env......"
export ZK_HOME=/home/hduser/hadoop/zookeeper-3.4.10
echo "config zookeeper env......"
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin
使环境变量生效：
source ~/.bashrc

（4）、创建事务日志存储路径dataDir：
sudo mkdir /home/hduser/data/zookeeper
sudo chown -R hduser /home/hduser/data/zookeeper/  #没有权限运行出错
进入目录，创建myid文件，文件写入id值为1，与zoo.cfg设置的server1:2888:3888对应，代表不同的zk节点：
sudo vi /home/hduser/data/zookeeper/myid

（5）、拷贝环境到其他服务器：
拷贝zk：
sudo scp -r ../zookeeper-3.4.10/ slave1:/home/hduser/hadoop/
sudo scp -r ../zookeeper-3.4.10/ slave2:/home/hduser/hadoop/
拷贝bashrc：
sudo scp -r ~/.bashrc slave1:/home/hduser/.bashrc
sudo scp -r ~/.bashrc slave2:/home/hduser/.bashrc

（6）、进入其他服务器重复步骤四；

（7）、运行：
zkServer.sh  start运行zk(每台都要执行)，出现一下输出代表安装成功，jps查看会出现QuorumPeerMain进程：
ZooKeeper JMX enabled by default
Using config: /home/hduser/hadoop/zookeeper-3.4.10/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

（8）、zkCli.sh模拟连接，quit退出连接。

7、初始化HA服务
（1）、启动Zookeeper：
在安装Zookeeper的服务器启动
zkServer.sh start
jps查看是否启动成功

（2）、所有服务器启动journalnode以完成NameNode数据同步：
hadoop-daemon.sh start journalnode
jps查看是否启动成功

（3）、主备节点元数据同步，第一次启动必须在选定的active节点格式化NameNode：
选择master主机：
hdfs namenode -format
启动主节点：
hadoop-daemon.sh start namenode
格式化之后到备份节点master-back，拷贝元数据到备份节点：
hdfs namenode -bootstrapStandby

(4)、初始化HA状态，即在Zookeeper创建节点：
hdfs zkfc -formatZK

（5）、启动集群：
start-all.sh

（6）、NameNode HA操作完之后我们可以发现只有一个节点（这里是master）启动，需要手动启动另外一个节点（master-back）的resourcemanager。
yarn-daemon.sh start resourcemanager

然后用以下指令查看resourcemanager状态
yarn rmadmin –getServiceState rm1

结果显示Active，而rm2是standby。

8、启动HA服务：
（1）、启动Zookeeper：
在安装Zookeeper的服务器启动
zkServer.sh start
jps查看是否启动成功
（2）、启动集群：
start-all.sh
